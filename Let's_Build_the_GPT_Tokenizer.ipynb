{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnxl_KqzJHXy",
        "outputId": "c4afdc90-08d0-4fd5-b22d-d078cb33d4ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique characters in the corpus : ' !,Tadehilmnorstwx' with a length of 18 characters\n"
          ]
        }
      ],
      "source": [
        "# A basic demonstration of the character-level tokenizer\n",
        "import torch\n",
        "text = 'This is some text dataset hello, and hi some words!'\n",
        "\n",
        "# By tokenizer, I am looking to map the input and the output layer\n",
        "# of the LLM or any other NLP system\n",
        "uniqueTextList = sorted(list(set(text)))\n",
        "uniqueText = ''.join(uniqueTextList)\n",
        "print(f\"Unique characters in the corpus : '{uniqueText}' with a length of {len(uniqueText)} characters\")\n",
        "\n",
        "# Encoder part\n",
        "stoi = {ch:i for i,ch in enumerate(uniqueText)}\n",
        "encoder = lambda x: [stoi[c] for c in x]\n",
        "\n",
        "# Decoder part\n",
        "itos = {i:ch for i,ch in enumerate(uniqueText)}\n",
        "decoder = lambda x: [itos[i] for i in x]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbLbShh6XdD9",
        "outputId": "b71e039a-e21d-4a6c-f97a-68eee072c77d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3, 7, 8, 14, 0, 8, 14, 0, 14, 12, 10, 6, 0, 15, 6, 17, 15, 0, 5, 4, 15, 4, 14, 6, 15, 0, 7, 6, 9, 9, 12, 2, 0, 4, 11, 5, 0, 7, 8, 0, 14, 12, 10, 6, 0, 16, 12, 13, 5, 14, 1]\n",
            "['T', 'h', 'i', 's', ' ', 'i', 's', ' ', 's', 'o', 'm', 'e', ' ', 't', 'e', 'x', 't', ' ', 'd', 'a', 't', 'a', 's', 'e', 't', ' ', 'h', 'e', 'l', 'l', 'o', ',', ' ', 'a', 'n', 'd', ' ', 'h', 'i', ' ', 's', 'o', 'm', 'e', ' ', 'w', 'o', 'r', 'd', 's', '!']\n",
            "This is some text dataset hello, and hi some words!\n"
          ]
        }
      ],
      "source": [
        "# Testing out the encoder and the decoder part\n",
        "# Tokenized text\n",
        "encodedText = encoder(text)\n",
        "print(encodedText)\n",
        "\n",
        "# Decoding the same text\n",
        "decodedText = decoder(encodedText)\n",
        "print(decodedText)\n",
        "\n",
        "# Jotting up the decoded text\n",
        "print(''.join(decodedText))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yewK-yEPGwtr"
      },
      "source": [
        "**In the code cell below, we would be exploring the reasons for NOT using utf encoding for tokenization in LLMS or any other NLP based transformers**\n",
        "\n",
        "*   Variability: UTF encoding scheme changes rapidly and therefore we will be continuously having to change the training of the tokenizer on the corpus\n",
        "*   Sparsity: UTF-16 and UTF-32 adds a lot of sparsity specially with the ASCII characters\n",
        "*   Long Sequences: UTF-8 has only 256 unique embeddings in the embedding table and therefore it will have long sequences making it tougher for the transformer to learn dependency over the tokens\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIxgszcmAxom",
        "outputId": "be8a7f77-cc7f-4946-891d-4e90d193dfa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UTF-8: [117, 122, 97, 105, 114, 32, 240, 159, 164, 151]\n",
            "Length of utf-8 bytes : 10\n",
            "UTF-16: [255, 254, 117, 0, 122, 0, 97, 0, 105, 0, 114, 0, 32, 0, 62, 216, 23, 221]\n",
            "Length of utf-16 bytes : 18\n",
            "UTF-32: [255, 254, 0, 0, 117, 0, 0, 0, 122, 0, 0, 0, 97, 0, 0, 0, 105, 0, 0, 0, 114, 0, 0, 0, 32, 0, 0, 0, 23, 249, 1, 0]\n",
            "Length of utf-32 bytes : 32\n"
          ]
        }
      ],
      "source": [
        "# Now, we are exploring the encode() method on python strings with the concept\n",
        "# that these can be used for encoding into the desired utf-xx format\n",
        "#print(\"uzair\".encode(\"utf-8\"))\n",
        "\n",
        "#text = \"ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ hello world ðŸ¤—\"\n",
        "text = \"uzair ðŸ¤—\"\n",
        "\n",
        "# UTF-8 encoding\n",
        "utf8_bytes = list(text.encode('utf-8'))\n",
        "print(f\"UTF-8: {utf8_bytes}\")\n",
        "print(f\"Length of utf-8 bytes : {len(utf8_bytes)}\")\n",
        "\n",
        "# UTF-16 encoding\n",
        "utf16_bytes = list(text.encode('utf-16'))\n",
        "print(f\"UTF-16: {utf16_bytes}\")\n",
        "print(f\"Length of utf-16 bytes : {len(utf16_bytes)}\")\n",
        "\n",
        "# UTF-32 encoding\n",
        "utf32_bytes = list(text.encode('utf-32'))\n",
        "print(f\"UTF-32: {utf32_bytes}\")\n",
        "print(f\"Length of utf-32 bytes : {len(utf32_bytes)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm2w_62HQaMk"
      },
      "source": [
        "# Implementing the Byte Pair Encoding (BPE) algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSV83MVvGxf4",
        "outputId": "3595c6eb-9d31-44d3-f294-174424fab12f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: ï¼µï½Žï½‰ï½ƒï½ï½„ï½…! ðŸ…¤ðŸ…ðŸ…˜ðŸ…’ðŸ…žðŸ…“ðŸ…”â€½ ðŸ‡ºâ€ŒðŸ‡³â€ŒðŸ‡®â€ŒðŸ‡¨â€ŒðŸ‡´â€ŒðŸ‡©â€ŒðŸ‡ª! ðŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to \"support Unicode\" in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don't blame programmers for still finding the whole thing mysterious, even 30 years after Unicode's inception.\n",
            "Length in characters: 533\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Get the sample text from Nathan Reed's blog post\n",
        "text = \"\"\"ï¼µï½Žï½‰ï½ƒï½ï½„ï½…! ðŸ…¤ðŸ…ðŸ…˜ðŸ…’ðŸ…žðŸ…“ðŸ…”â€½ ðŸ‡ºâ€ŒðŸ‡³â€ŒðŸ‡®â€ŒðŸ‡¨â€ŒðŸ‡´â€ŒðŸ‡©â€ŒðŸ‡ª! ðŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to \"support Unicode\" in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don't blame programmers for still finding the whole thing mysterious, even 30 years after Unicode's inception.\"\"\"\n",
        "\n",
        "print(f\"Text: {text}\")\n",
        "print(f\"Length in characters: {len(text)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bnf4xPCQ9hG",
        "outputId": "486a3d15-62be-4a60-dc11-522dd22d7254"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[239,\n",
              " 188,\n",
              " 181,\n",
              " 239,\n",
              " 189,\n",
              " 142,\n",
              " 239,\n",
              " 189,\n",
              " 137,\n",
              " 239,\n",
              " 189,\n",
              " 131,\n",
              " 239,\n",
              " 189,\n",
              " 143,\n",
              " 239,\n",
              " 189,\n",
              " 132,\n",
              " 239,\n",
              " 189,\n",
              " 133,\n",
              " 33,\n",
              " 32,\n",
              " 240,\n",
              " 159,\n",
              " 133,\n",
              " 164,\n",
              " 240,\n",
              " 159,\n",
              " 133,\n",
              " 157,\n",
              " 240,\n",
              " 159,\n",
              " 133,\n",
              " 152,\n",
              " 240,\n",
              " 159,\n",
              " 133,\n",
              " 146,\n",
              " 240,\n",
              " 159,\n",
              " 133,\n",
              " 158,\n",
              " 240,\n",
              " 159,\n",
              " 133,\n",
              " 147,\n",
              " 240,\n",
              " 159,\n",
              " 133,\n",
              " 148,\n",
              " 226,\n",
              " 128,\n",
              " 189,\n",
              " 32,\n",
              " 240,\n",
              " 159,\n",
              " 135,\n",
              " 186,\n",
              " 226,\n",
              " 128,\n",
              " 140,\n",
              " 240,\n",
              " 159,\n",
              " 135,\n",
              " 179,\n",
              " 226,\n",
              " 128,\n",
              " 140,\n",
              " 240,\n",
              " 159,\n",
              " 135,\n",
              " 174,\n",
              " 226,\n",
              " 128,\n",
              " 140,\n",
              " 240,\n",
              " 159,\n",
              " 135,\n",
              " 168,\n",
              " 226,\n",
              " 128,\n",
              " 140,\n",
              " 240,\n",
              " 159,\n",
              " 135,\n",
              " 180,\n",
              " 226,\n",
              " 128,\n",
              " 140,\n",
              " 240,\n",
              " 159,\n",
              " 135,\n",
              " 169,\n",
              " 226,\n",
              " 128,\n",
              " 140,\n",
              " 240,\n",
              " 159,\n",
              " 135,\n",
              " 170,\n",
              " 33,\n",
              " 32,\n",
              " 240,\n",
              " 159,\n",
              " 152,\n",
              " 132,\n",
              " 32,\n",
              " 84,\n",
              " 104,\n",
              " 101,\n",
              " 32,\n",
              " 118,\n",
              " 101,\n",
              " 114,\n",
              " 121,\n",
              " 32,\n",
              " 110,\n",
              " 97,\n",
              " 109,\n",
              " 101,\n",
              " 32,\n",
              " 115,\n",
              " 116,\n",
              " 114,\n",
              " 105,\n",
              " 107,\n",
              " 101,\n",
              " 115,\n",
              " 32,\n",
              " 102,\n",
              " 101,\n",
              " 97,\n",
              " 114,\n",
              " 32,\n",
              " 97,\n",
              " 110,\n",
              " 100,\n",
              " 32,\n",
              " 97,\n",
              " 119,\n",
              " 101,\n",
              " 32,\n",
              " 105,\n",
              " 110,\n",
              " 116,\n",
              " 111,\n",
              " 32,\n",
              " 116,\n",
              " 104,\n",
              " 101,\n",
              " 32,\n",
              " 104,\n",
              " 101,\n",
              " 97,\n",
              " 114,\n",
              " 116,\n",
              " 115,\n",
              " 32,\n",
              " 111,\n",
              " 102,\n",
              " 32,\n",
              " 112,\n",
              " 114,\n",
              " 111,\n",
              " 103,\n",
              " 114,\n",
              " 97,\n",
              " 109,\n",
              " 109,\n",
              " 101,\n",
              " 114,\n",
              " 115,\n",
              " 32,\n",
              " 119,\n",
              " 111,\n",
              " 114,\n",
              " 108,\n",
              " 100,\n",
              " 119,\n",
              " 105,\n",
              " 100,\n",
              " 101,\n",
              " 46,\n",
              " 32,\n",
              " 87,\n",
              " 101,\n",
              " 32,\n",
              " 97,\n",
              " 108,\n",
              " 108,\n",
              " 32,\n",
              " 107,\n",
              " 110,\n",
              " 111,\n",
              " 119,\n",
              " 32,\n",
              " 119,\n",
              " 101,\n",
              " 32,\n",
              " 111,\n",
              " 117,\n",
              " 103,\n",
              " 104,\n",
              " 116,\n",
              " 32,\n",
              " 116,\n",
              " 111,\n",
              " 32,\n",
              " 34,\n",
              " 115,\n",
              " 117,\n",
              " 112,\n",
              " 112,\n",
              " 111,\n",
              " 114,\n",
              " 116,\n",
              " 32,\n",
              " 85,\n",
              " 110,\n",
              " 105,\n",
              " 99,\n",
              " 111,\n",
              " 100,\n",
              " 101,\n",
              " 34,\n",
              " 32,\n",
              " 105,\n",
              " 110,\n",
              " 32,\n",
              " 111,\n",
              " 117,\n",
              " 114,\n",
              " 32,\n",
              " 115,\n",
              " 111,\n",
              " 102,\n",
              " 116,\n",
              " 119,\n",
              " 97,\n",
              " 114,\n",
              " 101,\n",
              " 32,\n",
              " 40,\n",
              " 119,\n",
              " 104,\n",
              " 97,\n",
              " 116,\n",
              " 101,\n",
              " 118,\n",
              " 101,\n",
              " 114,\n",
              " 32,\n",
              " 116,\n",
              " 104,\n",
              " 97,\n",
              " 116,\n",
              " 32,\n",
              " 109,\n",
              " 101,\n",
              " 97,\n",
              " 110,\n",
              " 115,\n",
              " 226,\n",
              " 128,\n",
              " 148,\n",
              " 108,\n",
              " 105,\n",
              " 107,\n",
              " 101,\n",
              " 32,\n",
              " 117,\n",
              " 115,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 32,\n",
              " 119,\n",
              " 99,\n",
              " 104,\n",
              " 97,\n",
              " 114,\n",
              " 95,\n",
              " 116,\n",
              " 32,\n",
              " 102,\n",
              " 111,\n",
              " 114,\n",
              " 32,\n",
              " 97,\n",
              " 108,\n",
              " 108,\n",
              " 32,\n",
              " 116,\n",
              " 104,\n",
              " 101,\n",
              " 32,\n",
              " 115,\n",
              " 116,\n",
              " 114,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 115,\n",
              " 44,\n",
              " 32,\n",
              " 114,\n",
              " 105,\n",
              " 103,\n",
              " 104,\n",
              " 116,\n",
              " 63,\n",
              " 41,\n",
              " 46,\n",
              " 32,\n",
              " 66,\n",
              " 117,\n",
              " 116,\n",
              " 32,\n",
              " 85,\n",
              " 110,\n",
              " 105,\n",
              " 99,\n",
              " 111,\n",
              " 100,\n",
              " 101,\n",
              " 32,\n",
              " 99,\n",
              " 97,\n",
              " 110,\n",
              " 32,\n",
              " 98,\n",
              " 101,\n",
              " 32,\n",
              " 97,\n",
              " 98,\n",
              " 115,\n",
              " 116,\n",
              " 114,\n",
              " 117,\n",
              " 115,\n",
              " 101,\n",
              " 44,\n",
              " 32,\n",
              " 97,\n",
              " 110,\n",
              " 100,\n",
              " 32,\n",
              " 100,\n",
              " 105,\n",
              " 118,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 32,\n",
              " 105,\n",
              " 110,\n",
              " 116,\n",
              " 111,\n",
              " 32,\n",
              " 116,\n",
              " 104,\n",
              " 101,\n",
              " 32,\n",
              " 116,\n",
              " 104,\n",
              " 111,\n",
              " 117,\n",
              " 115,\n",
              " 97,\n",
              " 110,\n",
              " 100,\n",
              " 45,\n",
              " 112,\n",
              " 97,\n",
              " 103,\n",
              " 101,\n",
              " 32,\n",
              " 85,\n",
              " 110,\n",
              " 105,\n",
              " 99,\n",
              " 111,\n",
              " 100,\n",
              " 101,\n",
              " 32,\n",
              " 83,\n",
              " 116,\n",
              " 97,\n",
              " 110,\n",
              " 100,\n",
              " 97,\n",
              " 114,\n",
              " 100,\n",
              " 32,\n",
              " 112,\n",
              " 108,\n",
              " 117,\n",
              " 115,\n",
              " 32,\n",
              " 105,\n",
              " 116,\n",
              " 115,\n",
              " 32,\n",
              " 100,\n",
              " 111,\n",
              " 122,\n",
              " 101,\n",
              " 110,\n",
              " 115,\n",
              " 32,\n",
              " 111,\n",
              " 102,\n",
              " 32,\n",
              " 115,\n",
              " 117,\n",
              " 112,\n",
              " 112,\n",
              " 108,\n",
              " 101,\n",
              " 109,\n",
              " 101,\n",
              " 110,\n",
              " 116,\n",
              " 97,\n",
              " 114,\n",
              " 121,\n",
              " 32,\n",
              " 97,\n",
              " 110,\n",
              " 110,\n",
              " 101,\n",
              " 120,\n",
              " 101,\n",
              " 115,\n",
              " 44,\n",
              " 32,\n",
              " 114,\n",
              " 101,\n",
              " 112,\n",
              " 111,\n",
              " 114,\n",
              " 116,\n",
              " 115,\n",
              " 44,\n",
              " 32,\n",
              " 97,\n",
              " 110,\n",
              " 100,\n",
              " 32,\n",
              " 110,\n",
              " 111,\n",
              " 116,\n",
              " 101,\n",
              " 115,\n",
              " 32,\n",
              " 99,\n",
              " 97,\n",
              " 110,\n",
              " 32,\n",
              " 98,\n",
              " 101,\n",
              " 32,\n",
              " 109,\n",
              " 111,\n",
              " 114,\n",
              " 101,\n",
              " 32,\n",
              " 116,\n",
              " 104,\n",
              " 97,\n",
              " 110,\n",
              " 32,\n",
              " 97,\n",
              " 32,\n",
              " 108,\n",
              " 105,\n",
              " 116,\n",
              " 116,\n",
              " 108,\n",
              " 101,\n",
              " 32,\n",
              " 105,\n",
              " 110,\n",
              " 116,\n",
              " 105,\n",
              " 109,\n",
              " 105,\n",
              " 100,\n",
              " 97,\n",
              " 116,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 46,\n",
              " 32,\n",
              " 73,\n",
              " 32,\n",
              " 100,\n",
              " 111,\n",
              " 110,\n",
              " 39,\n",
              " 116,\n",
              " 32,\n",
              " 98,\n",
              " 108,\n",
              " 97,\n",
              " 109,\n",
              " 101,\n",
              " 32,\n",
              " 112,\n",
              " 114,\n",
              " 111,\n",
              " 103,\n",
              " 114,\n",
              " 97,\n",
              " 109,\n",
              " 109,\n",
              " 101,\n",
              " 114,\n",
              " 115,\n",
              " 32,\n",
              " 102,\n",
              " 111,\n",
              " 114,\n",
              " 32,\n",
              " 115,\n",
              " 116,\n",
              " 105,\n",
              " 108,\n",
              " 108,\n",
              " 32,\n",
              " 102,\n",
              " 105,\n",
              " 110,\n",
              " 100,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 32,\n",
              " 116,\n",
              " 104,\n",
              " 101,\n",
              " 32,\n",
              " 119,\n",
              " 104,\n",
              " 111,\n",
              " 108,\n",
              " 101,\n",
              " 32,\n",
              " 116,\n",
              " 104,\n",
              " 105,\n",
              " 110,\n",
              " 103,\n",
              " 32,\n",
              " 109,\n",
              " 121,\n",
              " 115,\n",
              " 116,\n",
              " 101,\n",
              " 114,\n",
              " 105,\n",
              " 111,\n",
              " 117,\n",
              " 115,\n",
              " 44,\n",
              " 32,\n",
              " 101,\n",
              " 118,\n",
              " 101,\n",
              " 110,\n",
              " 32,\n",
              " 51,\n",
              " 48,\n",
              " 32,\n",
              " 121,\n",
              " 101,\n",
              " 97,\n",
              " 114,\n",
              " 115,\n",
              " 32,\n",
              " 97,\n",
              " 102,\n",
              " 116,\n",
              " 101,\n",
              " 114,\n",
              " 32,\n",
              " 85,\n",
              " 110,\n",
              " 105,\n",
              " 99,\n",
              " 111,\n",
              " 100,\n",
              " 101,\n",
              " 39,\n",
              " 115,\n",
              " 32,\n",
              " 105,\n",
              " 110,\n",
              " 99,\n",
              " 101,\n",
              " 112,\n",
              " 116,\n",
              " 105,\n",
              " 111,\n",
              " 110,\n",
              " 46]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 2: Encode the text to UTF-8 bytes and convert to list of integers\n",
        "utfEncodedString = list(text.encode(\"utf-8\"))\n",
        "#utfEncodedString, len(utfEncodedString), list(utfEncodedString)\n",
        "utfEncodedString"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Steps for implementing the BPE algorithm\n",
        "-- To generate bytes for the training corpus\n",
        "\n",
        "-- Iterate over the corpus to start minting new tokens by combining the most common and repeating sequences of bytes\n",
        "\n",
        "-- We keep on updating the vocabulary for all the new tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementing the statistics (# of occurence) for the tokens in the given corpus\n",
        "# This function will help us in our approach of minting new tokens and subsequently adding to the vocabulary\n",
        "from typing import List\n",
        "from typing import Dict\n",
        "def getStats(tokens : List[int], counts : Dict | None =None) -> Dict:\n",
        "    \n",
        "    # In case we don't have the existing dictionary of sequences, we are going to assume them as iniital step of the training\n",
        "    # The dictionary will therefore be empty\n",
        "    counts = {} if counts == None else counts\n",
        "\n",
        "    # Pairing them in the pythonic way excluding the last element\n",
        "    for pair in zip(tokens, tokens[1:]):\n",
        "        counts[pair] = counts.get(pair,0) + 1\n",
        "    \n",
        "    return counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 sorted pairs : [(20, (101, 32)), (15, (240, 159)), (12, (105, 110)), (10, (115, 32)), (10, (97, 110)), (10, (32, 97)), (9, (32, 116)), (8, (226, 128)), (8, (116, 104)), (7, (159, 135))]\n"
          ]
        }
      ],
      "source": [
        "# Getting the counts for the utf encoded string\n",
        "counts = getStats(utfEncodedString)\n",
        "\n",
        "# Sorting the list to ensure that we get the most repeating sequence being sorted to the lowest\n",
        "sortedCounts = sorted([(count, pairs) for pairs, count in counts.items()], reverse=True)\n",
        "print(f\"First 10 sorted pairs : {sortedCounts[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The most frequent pair is : (101, 32) and it appears to a total of 20 times\n",
            "Most frequent string : e \n"
          ]
        }
      ],
      "source": [
        "# Getting the most frequent pair from the dictionary\n",
        "mostFrequentPair = max(counts, key=counts.get)\n",
        "print(f\"The most frequent pair is : {mostFrequentPair} and it appears to a total of {counts[mostFrequentPair]} times\")\n",
        "\n",
        "# Converting these bytes back to the original string to see what these represents\n",
        "# chr : Takes in the unicode point and converts it back to the character\n",
        "# ord : Takes a character and then converts it to the unicode point\n",
        "mostFrequentString = chr(mostFrequentPair[0]) + chr(mostFrequentPair[1])\n",
        "print(f\"Most frequent string : {mostFrequentString}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iterating over the entire tokens (raw bytes) to find all the occurences of the most frequent token that is (101,32)\n",
        "# Once this has been done, we will use the next steps to mint tokens starting from the 256 ID\n",
        "from typing import Tuple\n",
        "def findOccurences(tokens: str, pair: Tuple) -> List[int]:\n",
        "    \"\"\"\n",
        "        tokens: These are the raw bytes for the corpus\n",
        "        pair: This is a tuple that needs to be located in the text corpus. \n",
        "\n",
        "        retuns: It returns a list of indices with the occurences\n",
        "    \"\"\"\n",
        "    occurences = []\n",
        "    for idx in range(len(tokens) - 1):\n",
        "        if tokens[idx] == pair[0] and tokens[idx + 1] == pair[1]:\n",
        "            occurences.append(idx)\n",
        "    \n",
        "    return occurences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[110,\n",
              " 120,\n",
              " 141,\n",
              " 150,\n",
              " 186,\n",
              " 198,\n",
              " 241,\n",
              " 269,\n",
              " 295,\n",
              " 325,\n",
              " 332,\n",
              " 362,\n",
              " 376,\n",
              " 384,\n",
              " 461,\n",
              " 466,\n",
              " 480,\n",
              " 508,\n",
              " 542,\n",
              " 548]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Getting the indices for all the occurences of the most frequent pair\n",
        "mostFrequentOccurences = findOccurences(utfEncodedString, mostFrequentPair)\n",
        "mostFrequentOccurences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Will replace pair (101, 32) with new token ID: 256\n",
            "Ready to implement merge function...\n"
          ]
        }
      ],
      "source": [
        "# Current tokens are 0-255 (256 possible values), so new token will be 256\n",
        "newTokenId = 256\n",
        "print(f\"Will replace pair {mostFrequentPair} with new token ID: {newTokenId}\")\n",
        "print(f\"Ready to implement merge function...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This function is the implementation of the new token minting\n",
        "def mintTokens(tokens: list[int],\n",
        "          indices: List[int],\n",
        "          newTokenId: int) -> List[int]:\n",
        "    \"\"\"\n",
        "        tokens: These are the raw bytes aka tokens list\n",
        "        indices: These are the lists of indices to be replaced (Pairs to be called new tokens)\n",
        "        newTokenId: This is the id that is going to replace at the desired index\n",
        "\n",
        "        returns: This is just a list of integers containing new tokens (with a minted one)\n",
        "    \"\"\"\n",
        "    newTokens = []\n",
        "    idx = 0\n",
        "    # Iterating over all the tokens length and making replacing the desired one with the new tokens\n",
        "    while idx < len(tokens):\n",
        "        if idx in indices:\n",
        "            newTokens.append(newTokenId)\n",
        "            idx += 2\n",
        "            continue\n",
        "        \n",
        "        newTokens.append(tokens[idx])\n",
        "        idx += 1\n",
        "\n",
        "    return newTokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing out our new token function with the findOccurences one to mint new tokens\n",
        "# We will potentially be adding a loop to continue this until we have either the \n",
        "# desired vocabulary size or the desired number of tokens\n",
        "\n",
        "\n",
        "# Moving in step by step... Getting the statistics\n",
        "tokenCount = getStats(utfEncodedString)\n",
        "\n",
        "# Get the most frequent pair\n",
        "mostFrequentPair = max(tokenCount, key=tokenCount.get)\n",
        "\n",
        "# Now getting the indices of the most frequent pair in our raw tokens list\n",
        "mostFrequentIndices = findOccurences(utfEncodedString, mostFrequentPair)\n",
        "\n",
        "# Minting new tokens\n",
        "newTokenId = 256\n",
        "newTokens = mintTokens(utfEncodedString, mostFrequentIndices, newTokenId)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(101, 32)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mostFrequentPair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(240, 256)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max(utfEncodedString), max(newTokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finidng the number of times we have got the new tokens\n",
        "# (It should match the number of times we have got the mostFrequentTokens count)\n",
        "assert newTokens.count(newTokenId) == len(mostFrequentIndices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(608, 588)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(utfEncodedString), len(newTokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original: [5, 6, 6, 7, 9, 1]\n",
            "After merging (6, 7) -> 99: [5, 99, 7, 9, 1]\n"
          ]
        }
      ],
      "source": [
        "# Test with simple example\n",
        "test_ids = [5, 6, 6, 7, 9, 1]\n",
        "result = mintTokens(test_ids, findOccurences(test_ids, (6,6)), 99)\n",
        "print(f\"Original: {test_ids}\")\n",
        "print(f\"After merging (6, 7) -> 99: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(20, 20)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Verifying that we don't have the mostFrequentPair anymore\n",
        "counts.get(mostFrequentPair, 0),newTokens.count(256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" Now, using the same merge function as that of Karpathy \"\"\"\n",
        "# Step 6: Implement the merge function\n",
        "def merge(ids, pair, idx):\n",
        "    \"\"\"\n",
        "    In the list of integers (ids), replace all consecutive occurrences \n",
        "    of pair with the new integer token idx\n",
        "    Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n",
        "    \"\"\"\n",
        "    newids = []\n",
        "    i = 0\n",
        "    while i < len(ids):\n",
        "        # if not at the very last position AND the pair matches, replace it\n",
        "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
        "            newids.append(idx)\n",
        "            i += 2  # skip over the pair\n",
        "        else:\n",
        "            newids.append(ids[i])\n",
        "            i += 1\n",
        "    return newids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BPE Training Progress:\n",
            "Step 0: 608 tokens, vocab size: 256\n",
            "Step 1: 588 tokens, vocab size: 257\n",
            "Step 2: 573 tokens, vocab size: 258\n",
            "  Merged pair: (240, 159) -> 257\n",
            "Step 3: 561 tokens, vocab size: 259\n",
            "  Merged pair: (105, 110) -> 258\n",
            "Step 4: 551 tokens, vocab size: 260\n",
            "  Merged pair: (115, 32) -> 259\n",
            "Step 5: 541 tokens, vocab size: 261\n",
            "  Merged pair: (97, 110) -> 260\n",
            "\n",
            "Final: 541 tokens, vocab size: 261\n"
          ]
        }
      ],
      "source": [
        "# Step 8: Iterate the BPE algorithm\n",
        "# Now we repeat: find most common pair, merge it, repeat...\n",
        "# Let's do a few more iterations\n",
        "tokens = utfEncodedString[:]\n",
        "current_tokens = newTokens[:]\n",
        "vocab_size = 257  # Started with 256, now have 257\n",
        "\n",
        "print(\"BPE Training Progress:\")\n",
        "print(f\"Step 0: {len(tokens)} tokens, vocab size: 256\")\n",
        "print(f\"Step 1: {len(current_tokens)} tokens, vocab size: {vocab_size}\")\n",
        "\n",
        "# Do a few more iterations\n",
        "for step in range(2, 6):  # Steps 2-5\n",
        "    # Find most common pair\n",
        "    stats = getStats(current_tokens)\n",
        "    if not stats:  # No more pairs to merge\n",
        "        break\n",
        "    \n",
        "    most_frequent_pair = max(stats, key=stats.get)\n",
        "    \n",
        "    # Merge it\n",
        "    current_tokens = merge(current_tokens, most_frequent_pair, vocab_size)\n",
        "    \n",
        "    print(f\"Step {step}: {len(current_tokens)} tokens, vocab size: {vocab_size + 1}\")\n",
        "    print(f\"  Merged pair: {most_frequent_pair} -> {vocab_size}\")\n",
        "    \n",
        "    vocab_size += 1\n",
        "\n",
        "print(f\"\\nFinal: {len(current_tokens)} tokens, vocab size: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token 256: (101, 32) -> 'e' + ' ' = 'e '\n",
            "Token 257: (100, 32) -> 'd' + ' ' = 'd '\n",
            "Token 258: (116, 101) -> 't' + 'e' = 'te'\n",
            "Token 259: (115, 32) -> 's' + ' ' = 's '\n",
            "Token 260: (105, 110) -> 'i' + 'n' = 'in'\n"
          ]
        }
      ],
      "source": [
        "# Track the merges we made\n",
        "merges = {\n",
        "    256: (101, 32),  # 'e' + ' '\n",
        "    257: (100, 32),  # 'd' + ' '  \n",
        "    258: (116, 101), # 't' + 'e'\n",
        "    259: (115, 32),  # 's' + ' '\n",
        "    260: (105, 110)  # 'i' + 'n'\n",
        "}\n",
        "\n",
        "for token_id, (byte1, byte2) in merges.items():\n",
        "    char1, char2 = chr(byte1), chr(byte2)\n",
        "    print(f\"Token {token_id}: ({byte1}, {byte2}) -> '{char1}' + '{char2}' = '{char1}{char2}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Ä'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chr(257)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" Implementing the decoder myself as mentioned by Karpathy to be an exercise\n",
        "    I WON'T BE SHYING AWAY FROM THE CHALLENGE :)\n",
        "\"\"\"\n",
        "\n",
        "# Challenge: Implement the decoder that takes in the list of integers and convert them back to the string\n",
        "def bpeDecoder(\n",
        "        tokens: List[int],\n",
        "        merges: Dict) -> str:\n",
        "    \n",
        "    \"\"\" token: This, I believe will be the initially generated raw tokens\n",
        "        margers: This is the dictionary that should have been included in the implementation of the BPE that holds keys and value pairs\n",
        "                KEYS: NEW TOKEN ID\n",
        "                VALUES: TUPLE CONTAINING THE MERGED TOKEN IDS\n",
        "    \"\"\"\n",
        "    for token in tokens:\n",
        "        # Check in case the outputs contain a new token and there we need to have \n",
        "\n",
        "# I was reading from the text and probably didn't understood the problem. \n",
        "# All we needed to do is to increase the vocabulary using a sort of itos lookup\n",
        "# Then I would be able to ensure that I am getting the output from the desired tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab[256] : b'e '\n",
            "vocab[257] : b'd '\n",
            "vocab[258] : b'te'\n",
            "vocab[259] : b's '\n",
            "vocab[260] : b'in'\n"
          ]
        }
      ],
      "source": [
        "# Track the merges we made\n",
        "merges = {\n",
        "    (101, 32) : 256,  # 'e' + ' '\n",
        "    (100, 32) : 257,  # 'd' + ' '  \n",
        "    (116, 101) : 258, # 't' + 'e'\n",
        "    (115, 32) : 259,  # 's' + ' '\n",
        "    (105, 110): 260  # 'i' + 'n'\n",
        "}\n",
        "# given ids (list of integers), return Python string\n",
        "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "for (p0, p1), idx in merges.items():\n",
        "    vocab[idx] = vocab[p0] + vocab[p1]\n",
        "    print(f\"vocab[{idx}] : {vocab[p0] + vocab[p1]}\")\n",
        "\n",
        "def decode(ids):\n",
        "    # given ids, get tokens\n",
        "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
        "    # convert from bytes to string\n",
        "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('ï¿½', b'\\x82')"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decode([130]), bytes([130])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "b'\\x80'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab[128]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Let's redeem this by taking on another challenge that is the encoder part "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode(corpus: str,\n",
        "            merges: Dict) -> List[int]:\n",
        "    \"\"\"\n",
        "        What we are looking to implement is that the transformation of the corpus into tokens\n",
        "        must follow the trained tokenizer\n",
        "    \"\"\"\n",
        "    # Converting the entire corpus into the byte sequence\n",
        "    tokens = list(corpus.encode(\"utf-8\"))\n",
        "\n",
        "    # Generating the reverse vocab (reverse emebdding table)\n",
        "    # reverseVocab = {token:tokenId for tokenId, token in vocab.items()}\n",
        "    \n",
        "    # Combining the tokens based on the vocabulary that we have developed over tokenizer training\n",
        "    while True:\n",
        "        idx = 0\n",
        "        outputTokens = []\n",
        "        isMergesLeft = False\n",
        "        while idx < len(tokens):\n",
        "            if (idx < len(tokens) - 1) and (tokens[idx], tokens[idx+1]) in merges:\n",
        "                outputTokens.append(merges[(tokens[idx], tokens[idx+1])])\n",
        "                idx += 2\n",
        "                isMergesLeft = True\n",
        "            else:\n",
        "                outputTokens.append(tokens[idx])\n",
        "                idx += 1\n",
        "        tokens = outputTokens[:]\n",
        "\n",
        "        if not isMergesLeft:\n",
        "            break\n",
        "\n",
        "    return outputTokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[77, 121, 32, 110, 97, 109, 256, 105, 259, 85, 122, 97, 105, 114, 32, 97, 110, 257, 73, 32, 97, 109, 32, 97, 32, 116, 111, 112, 32, 49, 37, 32, 77, 97, 99, 104, 260, 256, 76, 101, 97, 114, 110, 260, 103, 32, 101, 110, 103, 260, 101, 101, 114, 33]\n"
          ]
        }
      ],
      "source": [
        "print(encode(\"My name is Uzair and I am a top 1% Machine Learning engineer!\", merges))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My name is Uzair and I am a top 1% Machine Learning engineer!\n"
          ]
        }
      ],
      "source": [
        "# Testing the encoder and the decoder part\n",
        "print(decode(encode(\"My name is Uzair and I am a top 1% Machine Learning engineer!\", merges)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Problem : Sometimes in the corpus, there will be occurences such as \"dog,\", \"dog!\" and \"dog.\" etc. As mentioned in the GPT-2 paper \n",
        "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
        "\n",
        "a naive BPE algorithm in such case would be sub-optimal and hence we are looking for the regex implementation in the official GPT-2 repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['My', ' name', ' is', ' Uzair', ' and', ' I', ' am', ' a', ' top', ' 1', '%', ' Machine', ' Learning', ' engineer', '!']\n"
          ]
        }
      ],
      "source": [
        "import regex as re\n",
        "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
        "print(re.findall(gpt2pat, \"My name is Uzair and I am a top 1% Machine Learning engineer!\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT2 tokens : [220, 220, 220, 23748, 995, 10185]\n",
            "GPT4 tokens : [262, 24748, 1917, 12340]\n"
          ]
        }
      ],
      "source": [
        "# Tiktoken can let us analyze the tokenizers of various LLMs\n",
        "import tiktoken\n",
        "\n",
        "# GPT2 tokenizer does not merge spaces\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "print(f'GPT2 tokens : {enc.encode(\"    hello world!!!\")}')\n",
        "\n",
        "# GPT-4 tokenizer merges spaces as well\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "print(f'GPT4 tokens : {enc.encode(\"    hello world!!!\")}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([15339,\n",
              "  1917,\n",
              "  12340,\n",
              "  30,\n",
              "  320,\n",
              "  31495,\n",
              "  230,\n",
              "  75265,\n",
              "  243,\n",
              "  92245,\n",
              "  16715,\n",
              "  28509,\n",
              "  4513,\n",
              "  57037],\n",
              " 'hello world!!!? (ì•ˆë…•í•˜ì„¸ìš”!) lol123 ðŸ˜‰')"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# GPT-4 Tokenizer\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\") # this is the GPT-4 tokenizer\n",
        "ids = enc.encode(\"hello world!!!? (ì•ˆë…•í•˜ì„¸ìš”!) lol123 ðŸ˜‰\")\n",
        "text = enc.decode(ids) # get the same text back\n",
        "\n",
        "ids, text"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.10.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
